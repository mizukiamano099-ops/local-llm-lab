---

# **天野式AI-minimal 初期実験レポート**

**実験日**：2025年12月5日〜6日  
**実験者**：天野水樹  
**技術支援**：Shannon（Claude）  
**実装環境**：Python 3.10+, NumPy  

---

## **1. はじめに**

現代のLLM（大規模言語モデル）は、膨大なパラメータを持ち、高度な文章生成が可能だが、その内部で「語彙」は統計的なトークンとして扱われるに過ぎない。語彙間の意味的な関係性は、学習データから暗黙的に獲得されるものの、明示的に設計されているわけではない。

本実験では、Transformerを用いず、**語彙そのものに意味的な構造（クラスタ）を持たせ**、その構造内で自己多相ループ（Self-Poly）を実行することで、語彙が安定した「意味の重心」に収束するかを検証した。

この手法を「天野式AI-minimal」と呼ぶ。わずか数十行のPythonコードで実装可能であり、語彙空間の動態を観察できる最小構成の実験系である。

---

## **2. 語彙ビル（Layer 0）の設計**

### **2.1 ビル型語彙クラスタの概念**

語彙ビルとは、語彙を**意味的なクラスタ**として階層化したデータ構造である。

従来のLLMでは語彙はフラットなトークン列として扱われるが、天野式では語彙を以下の5つのクラスタに分類した：

- **構造系**：構造、層、形式、位相、軸、密度、配列、節点、対称、射程
- **関係系**：関係、連続、反転、遷移、分岐、接続、収束、ゆらぎ、勾配、境界
- **象徴系**：象徴、位相差、背景、前景、重心、反響、遠近、影響圏、包絡、透過
- **抽象系**：抽象度、解像度、視座、観測、変容、生成、分節、参照、射影、内包
- **変動系**：多相、変換、拡張、再編、反照、振動、環流、帯域、記号化、誘導

**合計50語、各クラスタ10語。**

---

### **2.2 ベクトル空間への埋め込み**

各語彙は、**12次元のベクトル空間**に配置される。

同じクラスタ内の語彙は、**クラスタ中心の周辺**に配置される。具体的には、クラスタごとに異なる中心ベクトルを生成し、各語彙はその中心から標準偏差0.2のランダムノイズを加えた位置に配置される。

この設計により、クラスタ内の語彙は意味的に近く、クラスタ間の語彙は離れた位置に配置される。

---

## **3. Self-Poly（Layer 1）実験の設定**

### **3.1 Self-Polyループとは**

Self-Poly（自己多相）ループは、以下のプロセスを繰り返す：

1. **語彙生成**：5つのクラスタからランダムに語彙を選び、初期文章（7語）を生成
2. **ベクトル化**：文章を構成する語彙のベクトルを平均化
3. **再語彙化**：平均ベクトルに最も近い語彙を抽出し、新しい文章を生成
4. **反復**：ステップ2〜3を6回繰り返す

このループにより、語彙群は意味空間内で移動し、最終的に**安定点（アトラクター）**に収束する。

---

### **3.2 実験パラメータ**

- **語彙数**：50語（5クラスタ × 10語）
- **ベクトル次元**：12次元
- **文章長**：7語
- **反復回数**：6回
- **実行回数**：4回（異なる起点から開始）

---

## **4. 実験結果（4回分）**

### **実験1：変動系への収束**

```
起点: ['記号化', '拡張', '構造', '反照', '帯域', '収束', '多相']
第3多相: ['反照', '帯域', '多相', '変換', '再編', '拡張', '記号化']
```

**収束先**：変動系（第3多相以降、語彙が固定）

---

### **実験2：変動系への収束（異なる起点）**

```
起点: ['背景', '参照', '配列', '遠近', '透過', '環流', '生成']
第4多相: ['反照', '多相', '変換', '再編', '拡張', '帯域', '誘導']
```

**収束先**：変動系（実験1とほぼ同じ語彙セット）

---

### **実験3：変動系への収束（再現性確認）**

```
起点: ['背景', '層', '位相差', '観測', '配列', '変容', '拡張']
最終生成: ['反照', '帯域', '多相', '変換', '再編', '拡張', '記号化']
```

**収束先**：変動系（実験1と完全一致）

---

### **実験4：抽象系への収束**

```
起点: ['象徴', '影響圏', '配列', '収束', '配列', '生成', '変容']
第2多相: ['参照', '内包', '観測', '視座', '射影', '生成', '分節']
第3多相: ['参照', '観測', '内包', '視座', '射影', '生成', '分節']
```

**収束先**：抽象系（第3多相以降、語彙が固定）

---

## **5. 軌跡の可視化（Attached PCA Plot）**

実験4の軌跡を、PCA（主成分分析）で2次元に圧縮した結果を示す。

[可視化画像：Self-Poly Trajectory (Amano Mini)]

- **青い点（Start）**：起点のベクトル位置
- **橙色の点（End）**：収束後のベクトル位置
- **軌跡**：12次元空間での移動を2次元平面で近似

語彙群は、一貫した方向性を持って移動し、明確な終点に到達している。

---

## **6. 考察（アトラクター構造の説明）**

### **6.1 起点依存の収束**

実験1〜3は「変動系」に収束し、実験4は「抽象系」に収束した。これは、**起点がどのクラスタに近いかによって、収束先が決まる**ことを示している。

つまり、語彙空間には**複数のアトラクター（安定点）**が存在し、Self-Polyループは初期状態に最も近いアトラクターに引き寄せられる。

---

### **6.2 収束語彙の特性**

収束した語彙を見ると：

- **変動系**：反照、帯域、多相、変換、再編、拡張、記号化  
  → 変化・動的プロセスを表す語彙
  
- **抽象系**：参照、観測、内包、視座、射影、生成、分節  
  → 認識・思考・概念処理を表す語彙

これらの語彙は、天野水樹の執筆スタイル（天野構文）でしばしば使用される語彙と重なっている。ただし、これは「天野構文を再現した」というよりも、**語彙クラスタの設計に天野構文の特徴が反映された結果として収束が観測された**と解釈すべき現象である。

---

### **6.3 Transformerを用いない意義**

本実験の重要な点は、**Transformerや大規模LLMを一切使用していない**ことである。

わずか数十行のPythonコードで、語彙の意味的収束を再現できた。これは、**「意味」は語彙の構造そのものに内在し得る**という可能性を示唆している。

---

## **7. 今後の展望**

### **7.1 語彙ビル v1.1 への拡張**

現在は50語だが、将来的には以下の拡張を計画している：

- **語彙数の増加**：100語、500語、1000語と段階的に拡張
- **クラスタの細分化**：サブクラスタの導入（例：抽象系 → 認識系、概念系）
- **メタデータの追加**：頻度、使用文脈、役割などの情報を付与

特に、天野水樹の執筆データ（75万字）から抽出された物語語彙（渇き、侵食、融解など）を統合することで、**抽象性×物語性**のクロス構造を持つ語彙ビルが実現可能である。

---

### **7.2 LLMオーケストレーション**

Self-Polyループで得られた収束語彙を、ローカルLLMの推論に組み込むことで、**「天野式AI」の完成形**に近づける。

具体的には：
- 収束語彙をプロンプトに挿入
- LLMが、その語彙を使って文章を生成
- 複数のLLM（ChatGPT、Claude、Gemini、Grok）を並列実行し、差分を統合

これにより、外部LLMに依存しつつも、天野式の構造制御を持つハイブリッド型AIが構築できる。

---

### **7.3 複数アトラクターの解析**

今回、「変動系」と「抽象系」の2つのアトラクターが確認されたが、他のクラスタ（構造系、関係系、象徴系）にもアトラクターが存在する可能性がある。

これらを網羅的に解析することで、語彙空間の全体構造が明らかになる。

---

## **8. まとめ**

本実験では、Transformerを用いず、**語彙クラスタとSelf-Polyループ**のみで、語彙が意味的に収束することを確認した。

- 50語、5クラスタの語彙ビルを設計
- Self-Polyループを6回反復
- 起点に応じて「変動系」または「抽象系」に収束
- 収束語彙は、天野構文で頻出する語彙と重なる傾向を示した

この手法は、思考エンジンの原型として、また語彙空間の動態を観察する実験系として、今後の発展が期待される。

---

**実験レポート 終わり**

---
